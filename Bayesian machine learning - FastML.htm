<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!-->
<html class="js video no-maskImage placeholder" lang="en"><!--<![endif]--><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Bayesian machine learning - FastML</title>
  <meta name="author" content="Zygmunt Z.">

	
	<meta name="description" content="So you know the Bayes rule. How does it relate to machine learning? It can be quite difficult to grasp how the puzzle pieces fit together - we know …">
	<meta name="keywords" content="machine learning, data analysis, data science, classification, regression, vowpal wabbit, spearmint, random forest">

	<!--
  
  <meta name="description" content="So you know the Bayes rule. How does it relate to machine learning? It can be quite difficult to grasp how the puzzle pieces fit together - we know &hellip;">
  
  -->

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width,minimum-scale=1,maximum-scale=1">

	<script src="Bayesian%20machine%20learning%20-%20FastML_files/cbgapi.loaded_1" async=""></script><script src="Bayesian%20machine%20learning%20-%20FastML_files/cbgapi.loaded_0" async=""></script><script type="text/javascript" async="" src="Bayesian%20machine%20learning%20-%20FastML_files/plusone.js" gapi_processed="true"></script><script type="text/javascript" async="" src="Bayesian%20machine%20learning%20-%20FastML_files/dc.js"></script><script src="Bayesian%20machine%20learning%20-%20FastML_files/jquery_002.js"></script>
	<script src="Bayesian%20machine%20learning%20-%20FastML_files/bootstrap.js"></script>
	<link href="Bayesian%20machine%20learning%20-%20FastML_files/bootstrap.css" rel="stylesheet">


  
  <link rel="canonical" href="http://fastml.com/bayesian-machine-learning">
  <link href="http://fastml.com/favicon.png" rel="icon">
  
  <link href="Bayesian%20machine%20learning%20-%20FastML_files/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="Bayesian%20machine%20learning%20-%20FastML_files/modernizr-2.js"></script>
  <script src="Bayesian%20machine%20learning%20-%20FastML_files/ender.js"></script>
  <script src="Bayesian%20machine%20learning%20-%20FastML_files/octopress.js" type="text/javascript"></script>
  <link href="http://fastml.com/atom.xml" rel="alternate" title="FastML" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="Bayesian%20machine%20learning%20-%20FastML_files/css_002.css" rel="stylesheet" type="text/css">
<link href="Bayesian%20machine%20learning%20-%20FastML_files/css.css" rel="stylesheet" type="text/css">

  
  <script src="Bayesian%20machine%20learning%20-%20FastML_files/jquery.js"></script>

<script src="Bayesian%20machine%20learning%20-%20FastML_files/api.js"></script>
<script type="text/javascript">
	var background = jQuery.cookie( "background" );
	//var variation = cxApi.chooseVariation();
</script>






  
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-452062-21']);
    _gaq.push(['_trackPageview']);
  </script>

  <script type="text/javascript" src="Bayesian%20machine%20learning%20-%20FastML_files/cookie.htm"></script>

  <script type="text/javascript">
    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      //ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      ga.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'stats.g.doubleclick.net/dc.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
  


  
  <script type="text/javascript">
	function set_background( url ) {
		document.body.style.backgroundImage = 'url(' + url + ')';
		jQuery.cookie( 'background', url, { expires: 256, path: '/' } );
		return false;
	}
	
	function setCustomBackground() {
		url = document.getElementById("custom_background_url").value;
		set_background( url );
		document.getElementById("backgroundModal").modal('hide');
	}	
</script>

<style type="text/css">
	ul.dropdown-menu a {
		text-decoration: none;
		//font-family: Arial;
		font-size: smaller;
	}	

	/* followers */	
	div.well {
		font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
		font-size: 14px;
		line-height: 20px;
		color: #333333;
		background-color: #ffffff;
	}		
</style>

<script type="text/javascript" async="" src="Bayesian%20machine%20learning%20-%20FastML_files/embed.js"></script><script type="text/javascript" async="" src="Bayesian%20machine%20learning%20-%20FastML_files/widgets.js"></script><script type="text/javascript" src="Bayesian%20machine%20learning%20-%20FastML_files/fastml.js" async=""></script><script type="text/javascript" charset="utf-8" async="" src="Bayesian%20machine%20learning%20-%20FastML_files/button.js"></script></head>

<body style="background-image: url('/images/wallpapers/session/julia_roberts_house.jpg');">
  <header role="banner"><hgroup>
  <h1><a href="http://fastml.com/">FastML</a></h1>
  
    <h2>Machine learning made easy</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="http://fastml.com/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input name="as_sitesearch" value="fastml.com" type="hidden">
    <input class="search" name="q" results="0" placeholder="Search" type="text">
  </fieldset><fieldset class="mobile-nav"><select><option value="">Navigate…</option><option value="http://fastml.com/">» Home</option><option value="http://fastml.com/contents/">» Contents</option><option value="http://fastml.com/popular/">» Popular</option><option value="http://fastml.com/links/">» Links</option><option value="http://fastml.com/backgrounds/">» Backgrounds</option><option value="http://fastml.com/about/">» About</option><option value="http://fastml.com/atom.xml" selected="selected">» RSS</option></select></fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="http://fastml.com/">Home</a></li>
  <li><a href="http://fastml.com/contents/">Contents</a></li>
  <li id="popular"><a href="http://fastml.com/popular/">Popular</a></li>
  <li><a href="http://fastml.com/links/">Links</a></li>
  <li id="backgrounds"><a href="http://fastml.com/backgrounds/">Backgrounds</a></li>
  <li id="about"><a href="http://fastml.com/about/">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Bayesian machine learning</h1>
    
    
      <p class="meta">
        








  


<time datetime="2016-03-28T21:25:00+02:00" pubdate="" data-updated="true">2016-03-28</time>
        
      </p>
    
  </header>


<div class="entry-content"><p>So you know the Bayes rule. How does it 
relate to machine learning? It can be quite difficult to grasp how the 
puzzle pieces fit together - we know it took us a while. This article is
 an introduction we wish we had back then.</p>

<!-- more -->


<p><meta property="twitter:card" content="summary">
<meta property="twitter:site" content="@fastml">
<meta property="twitter:title" content="Bayesian machine learning"></p>

<p><meta property="twitter:description" content="So you know the Bayes rule. How does it relate to machine learning? It can be quite difficult to grasp how the puzzle pieces fit together - we know it took us a while. This article is an introduction we wish we had back then."></p>

<p><meta property="twitter:image" content="http://fastml.com/images/bayesian-machine-learning/heteroscedastic_noise.png">
<meta property="twitter:url" content="http://fastml.com/bayesian-machine-learning/"></p>

<p><em>While we have some grasp on the matter, we’re not experts, so the
 following might contain inaccuracies or even outright errors. Feel free
 to point them out, either in the comments or privately.</em></p>

<h2>Bayesians and Frequentists</h2>

<p>In essence, Bayesian means probabilistic. The specific term exists 
because there are two approaches to probability. Bayesians think of it 
as a measure of belief, so that probability is subjective and refers to 
the future.</p>

<p>Frequentists have a different view: they use probability to refer to 
past events - in this way it’s objective and doesn’t depend on one’s 
beliefs. The name comes from the method - for example: we tossed a coin 
100 times, it came up heads 53 times, so the frequency/probability of 
heads is 0.53.</p>

<p>For a thorough investigation of this topic and more, refer to Jake VanderPlas’ <a href="https://jakevdp.github.io/blog/2015/08/07/frequentism-and-bayesianism-5-model-selection/">Frequentism and Bayesianism</a> series of articles.</p>

<h2>Priors, updates, and posteriors</h2>

<p>As Bayesians, we start with a belief, called a prior. Then we obtain 
some data and use it to update our belief. The outcome is called a 
posterior. Should we obtain even more data, the old posterior becomes a 
new prior and the cycle repeats.</p>

<p>This process employs the <strong>Bayes rule</strong>:</p>

<pre><code>P( A | B ) = P( B | A ) * P( A ) / P( B )
</code></pre>

<p><code>P( A | B )</code>, read as “probability of A given B”, indicates a conditional probability: how likely is A if B happens.</p>

<h2>Inferring model parameters from data</h2>

<p>In Bayesian machine learning we use the Bayes rule to infer model parameters (theta) from data (D):</p>

<pre><code>P( theta | D ) = P( D | theta ) * P( theta ) / P( data )
</code></pre>

<p>All components of this are probability distributions.</p>

<p><code>P( data )</code> is something we generally cannot compute, but 
since it’s just a normalizing constant, it doesn’t matter that much. 
When comparing models, we’re mainly interested in expressions containing
 theta, because <code>P( data )</code> stays the same for each model.</p>

<p><code>P( theta )</code> is a prior, or our belief of what the model 
parameters might be. Most often our opinion in this matter is rather 
vague and if we have enough data, we simply don’t care. Inference should
 converge to probable theta as long as it’s not zero in the prior. One 
specifies a prior in terms of a parametrized distribution - see <a href="http://zinkov.com/posts/2015-06-09-where-priors-come-from/">Where priors come from</a>.</p>

<p><code>P( D | theta )</code> is called likelihood of data given model 
parameters. The formula for likelihood is model-specific. People often 
use likelihood for evaluation of models: a model that gives higher 
likelihood to real data is better.</p>

<p>Finally, <code>P( theta | D )</code>, a posterior, is what we’re after. It’s a probability distribution over model parameters obtained from prior beliefs and data.</p>

<p>When one uses likelihood to get point estimates of model parameters, it’s called <a href="https://en.wikipedia.org/wiki/Maximum_likelihood">maximum-likelihood estimation</a>, or MLE. If one also takes the prior into account, then it’s <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">maximum a posteriori estimation</a> (MAP). MLE and MAP are the same if the prior is uniform.</p>

<p>Note that choosing a model can be seen as separate from choosing 
model (hyper)parameters. In practice, though, they are usually performed
 together, by validation.</p>

<h2>Model vs inference</h2>

<p>Inference refers to how you learn parameters of your model. A model 
is separate from how you train it, especially in the Bayesian world.</p>

<p>Consider deep learning: you can train a network using Adam, RMSProp 
or a number of other optimizers. However, they tend to be rather similar
 to each other, all being variants of Stochastic Gradient Descent. In 
contrast, Bayesian methods of inference differ from each other more 
profoundly.</p>

<p>The two most important methods are <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo sampling</a> and variational inference. Sampling is a gold standard, but slow. The <a href="http://fastml.com/an-excerpt-from-the-master-algorithm/">excerpt from The Master Algorithm</a> has more on MCMC.</p>

<p>Variational inference is a method designed explicitly to trade some 
accuracy for speed. It’s drawback is that it’s model-specific, but 
there’s light at the end of the tunnel - see the <a href="#software">section on software</a> below and <a href="http://arxiv.org/abs/1601.00670">Variational Inference: A Review for Statisticians</a>.</p>

<h2>Statistical modelling</h2>

<p>In the spectrum of Bayesian methods, there are two main flavours. Let’s call the first <em>statistical modelling</em> and the second <em>probabilistic machine learning</em>. The latter contains the so-called nonparametric approaches.</p>

<p>Modelling happens when data is scarce and precious and hard to 
obtain, for example in social sciences and other settings where it is 
difficult to conduct a large-scale controlled experiment. Imagine a 
statistician meticulously constructing and tweaking a model using what 
little data he has. In this setting you spare no effort to make the best
 use of available input.</p>

<p>Also, with small data it is important to quantify uncertainty and that’s precisely what Bayesian approach is good at.</p>

<p>Bayesian methods - specifically MCMC - are usually computationally costly. This again goes hand-in-hand with small data.</p>

<p>To get a taste, consider <a href="https://github.com/stan-dev/example-models/wiki/ARM-Models-Sorted-by-Type">examples</a> for the <a href="http://www.stat.columbia.edu/%7Egelman/arm/">Data Analysis Using Regression Analysis and Multilevel/Hierarchical Models</a>
 book. That’s a whole book on linear models. They start with a bang:  a 
linear model with no predictors, then go through a number of linear 
models with one predictor, two predictors, six predictors, up to eleven.</p>

<p>This labor-intensive mode goes against a current trend in machine 
learning to use data for a computer to learn automatically from it.</p>

<h2>Probabilistic machine learning</h2>

<p>Let’s try replacing “Bayesian” with “probabilistic”. From this 
perspective, it doesn’t differ as much from other methods. As far as 
classification goes, most classifiers are able to output probabilistic 
predictions. Even SVMs, which are sort of an antithesis of Bayesian.</p>

<p>By the way, these probabilities are only statements of belief from a 
classifier. Whether they correspond to real probabilities is another 
matter completely and it’s called <a href="http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression/">calibration</a>.</p>

<h3>LDA, a generative model</h3>

<p><a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirichlet Allocation</a>
 is a method that one throws data at and allows it to sort things out 
(as opposed to manual modelling). It’s similar to matrix factorization 
models, especially non-negative MF. You start with a matrix where rows 
are documents, columns are words and each element is a count of a given 
word in a given document. LDA “factorizes” this matrix of size <em>n x d</em> into two matrices, documents/topics (<em>n x k</em>) and topics/words (<em>k x d</em>).</p>

<p>The difference from factorization is that you can’t multiply those 
two matrices to get the original, but since the appropriate rows/columns
 sum to one, you can “generate” a document. To get the first word, one 
samples a topic, then a word from this topic (the second matrix). Repeat
 this for a number of words you want. Notice that this is a bag-of-words
 representation, not a proper sequence of words.</p>

<p>The above is an example of a <strong>generative</strong> model, meaning that one can sample, or generate examples, from it. Compare with classifiers, which usually model <code>P( y | x )</code> to discriminate between classes based on <em>x</em>. A generative model is concerned with joint distribution of <em>y</em> and <em>x</em>, <code>P( y, x )</code>. It’s more difficult to estimate that distribution, but it allows sampling and of course one can get <code>P( y | x )</code> from <code>P( y, x )</code>.</p>

<h2>Bayesian nonparametrics</h2>

<p>While there’s no exact definition, the name means that the number of 
parameters in a model can grow as more data become available. This is 
similar to Support Vector Machines, for example, where the algorithm 
chooses support vectors from the training points. Nonparametrics include
 Hierarchical Dirichlet Process version of LDA, where the number of 
topics chooses itself automatically, and Gaussian Processes.</p>

<h3>Gaussian Processes</h3>

<p>Gaussian processes are somewhat similar to Support Vector Machines - 
both use kernels and have similar scalability (which has been vastly 
improved throughout the years by using approximations). A natural 
formulation for GP is regression, with classification as an 
afterthought. For SVM it’s the other way around.</p>

<p>Another difference is that GP are probabilistic from the ground up 
(providing error bars), while SVM are not. You can observe this in 
regression. Most “normal” methods only provide point estimates. Bayesian
 counterparts, like Gaussian processes, also output uncertainty 
estimates.</p>

<p><img src="Bayesian%20machine%20learning%20-%20FastML_files/homoscedastic_noise.png" alt=""><br>
<span style="font-size: smaller;">Credit: Yarin Gal’s <a href="https://github.com/yaringal/HeteroscedasticDropoutUncertainty">Heteroscedastic dropout uncertainty
</a> and <a href="http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html">What my deep model doesn’t know</a></span></p>

<p>Unfortunately, it’s not the end of the story. Even a sophisticated 
method like GP normally operates on an assumption of homoscedasticity, 
that is, uniform noise levels. In reality, noise might differ across 
input space (be heteroscedastic) - see the image below.</p>

<p><img src="Bayesian%20machine%20learning%20-%20FastML_files/heteroscedastic_noise.png" alt=""></p>

<p>A relatively popular application of Gaussian Processes is 
hyperparameter optimization for machine learning algorithms. The data is
 small, both in dimensionality - usually only a few parameters to tweak,
 and in the number of examples. Each example represents one run of the 
target algorithm, which might take hours or days. Therefore we’d like to
 get to the good stuff with as few examples as possible.</p>

<p>Most of the research on GP seems to happen in Europe. English have 
done some interesting work on making GP easier to use, culminating in 
the <a href="http://www.automaticstatistician.com/index/">automated statistician</a>, a project led by Zoubin Ghahramani.</p>

<p>Watch the first 10 minutes of this <a href="https://www.youtube.com/watch?v=BS4Wd5rwNwE">video</a> for an accessible intro to Gaussian Processes.</p>

<p><a name="software"></a></p>

<h2>Software</h2>

<p>The most conspicuous piece of Bayesian software these days is probably <a href="http://mc-stan.org/">Stan</a>.
 Stan is a probabilistic programming language, meaning that it allows 
you to specify and train whatever Bayesian models you want. It runs in 
Python, R and other languages. Stan has a modern sampler called <a href="http://andrewgelman.com/2011/11/30/stan-uses-nuts/">NUTS</a>:</p>

<blockquote><p>Most of the computation [in Stan] is done using 
Hamiltonian Monte Carlo. HMC requires some tuning, so Matt Hoffman up 
and wrote a new algorithm, Nuts (the “No-U-Turn Sampler”) which 
optimizes HMC adaptively. In many settings, Nuts is actually more 
computationally efficient than the optimal static HMC!</p></blockquote>

<p>One especially interesting thing about Stan is that it has <a href="http://arxiv.org/abs/1506.03431">automatic variational inference</a>:</p>

<blockquote><p>Variational inference is a scalable technique for 
approximate Bayesian inference. Deriving variational inference 
algorithms requires tedious model-specific calculations; this makes it 
difficult to automate. We propose an automatic variational inference 
algorithm, automatic differentiation variational inference (ADVI). The 
user only provides a Bayesian model and a dataset; nothing else.</p></blockquote>

<p>This technique paves way to applying small-style modelling to at least medium-sized data.</p>

<p>In Python, the most popular package is <a href="https://pymc-devs.github.io/pymc3/">PyMC</a>.
 It is not as advanced or polished (the developers seem to be playing 
catch-up with Stan), but still good. PyMC has NUTS and ADVI - here’s a 
notebook with a <a href="https://gist.github.com/taku-y/b4da34be310718a6ea02">minibatch ADVI example</a>. The software uses Theano as a backend, so it’s faster than pure Python.</p>

<p><a href="http://research.microsoft.com/en-us/um/cambridge/projects/infernet/">Infer.NET</a>
 is Microsoft’s library for probabilistic programming. It’s mainly 
available from languages like C# and F#, but apparently can also be 
called from .NET’s IronPython. Infer.net uses <a href="http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Working%20with%20different%20inference%20algorithms.aspx">expectation propagation</a> by default.</p>

<p>Besides those, there’s a myriad of packages implementing various 
flavours of Bayesian computing, from other probabilistic programming 
languages to specialized LDA implementations. One interesting example is
 <a href="https://github.com/probcomp/crosscat">CrossCat</a>:</p>

<blockquote><p>CrossCat is a domain-general, Bayesian method for 
analyzing high-dimensional data tables. CrossCat estimates the full 
joint distribution over the variables in the table from the data, via 
approximate inference in a hierarchical, nonparametric Bayesian model, 
and provides efficient samplers for every conditional distribution. 
CrossCat combines strengths of nonparametric mixture modeling and 
Bayesian network structure learning: it can model any joint distribution
 given enough data by positing latent variables, but also discovers 
independencies between the observable variables.</p></blockquote>

<p>and <a href="http://probcomp.csail.mit.edu/bayesdb/">BayesDB</a>/<a href="https://github.com/probcomp/bayeslite">Bayeslite</a> from the same people.</p>

<h2>Resources</h2>

<p>To solidify your understanding, you might go through Radford Neal’s tutorial on <a href="http://www.cs.toronto.edu/%7Eradford/ftp/bayes-tut.pdf">Bayesian Methods for Machine Learning</a>. It corresponds 1:1 to the subject of this post.</p>

<p>We found Kruschke’s <a href="https://sites.google.com/site/doingbayesiandataanalysis/what-s-new-in-2nd-ed">Doing Bayesian Data Analysis</a>, known as the puppy book, most readable. The author goes to great lengths to explain all the ins and outs of modelling.</p>

<p><a href="http://xcelab.net/rm/statistical-rethinking/">Statistical rethinking</a> appears to be of the similar kind, but newer. It has examples in R + Stan. The author, Richard McElreath, published a <a href="https://www.youtube.com/playlist?list=PLDcUM9US4XdMdZOhJWJJD4mDBMnbTWw_z">series of lectures</a> on YouTube.</p>

<p>In terms of machine learning, both books only only go as far as linear models. Likewise, Cam Davidson-Pylon’s <a href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/#contents">Probabilistic Programming &amp; Bayesian Methods for Hackers</a> covers the <em>Bayesian</em> part, but not the <em>machine learning</em> part.</p>

<p>The same goes to Alex Etz’ series of articles on <a href="http://alexanderetz.com/understanding-bayes/">understanding Bayes</a>.</p>

<p>For those mathematically inclined, <a href="https://www.cs.ubc.ca/%7Emurphyk/MLbook/">Machine Learning: a Probabilistic Perspective</a> by Kevin Murphy might be a good book to check out. You like hardcore? No problemo, Bishop’s <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Pattern Recognition and Machine Learning</a> got you covered. One recent <a href="https://www.reddit.com/r/MachineLearning/comments/4c5gqk/is_pattern_recognition_and_machine_learning_still/">Reddit thread</a> briefly discusses these two.</p>

<p><a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage">Bayesian Reasoning and Machine Learning</a> by David Barber is also popular, and freely available online, as is <a href="http://www.gaussianprocess.org/gpml/">Gaussian Processes for Machine Learning</a>, the classic book on the matter.</p>

<p>As far as we know, there’s no MOOC on Bayesian machine learning, but <em>mathematicalmonk</em> explains <a href="https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA">machine learning</a> from the Bayesian perspective.</p>

<p>Stan has an extensive <a href="http://mc-stan.org/documentation/">manual</a>, PyMC a <a href="https://pymc-devs.github.io/pymc3/getting_started.html">tutorial</a> and quite a few examples.</p>
</div>


  
  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Zygmunt Z.</span></span>

      








  


<time datetime="2016-03-28T21:25:00+02:00" pubdate="" data-updated="true">2016-03-28</time>
      

<span class="categories">
  
    <a class="category" href="http://fastml.com/blog/categories/basics/">basics</a>, <a class="category" href="http://fastml.com/blog/categories/software/">software</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <iframe id="twitter-widget-1" scrolling="no" allowtransparency="true" class="twitter-share-button twitter-share-button-rendered twitter-tweet-button" style="position: static; visibility: visible; width: 63px; height: 20px;" title="Twitter Tweet Button" src="Bayesian%20machine%20learning%20-%20FastML_files/tweet_button.htm" data-url="http://fastml.com/bayesian-machine-learning/" frameborder="0"></iframe>
  
  
  <div style="text-indent: 0px; margin: 0px; padding: 0px; background: transparent none repeat scroll 0% 0%; border-style: none; float: none; line-height: normal; font-size: 1px; vertical-align: baseline; display: inline-block; width: 90px; height: 20px;" id="___plusone_0"><iframe ng-non-bindable="" hspace="0" marginheight="0" marginwidth="0" scrolling="no" style="position: static; top: 0px; width: 90px; margin: 0px; border-style: none; left: 0px; visibility: visible; height: 20px;" tabindex="0" vspace="0" id="I0_1491178111776" name="I0_1491178111776" src="Bayesian%20machine%20learning%20-%20FastML_files/fastbutton.htm" data-gapiattached="true" title="+1" frameborder="0" width="100%"></iframe></div>
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="http://fastml.com/what-next/" title="Previous Post: What next?">« What next?</a>
      
      
        <a class="basic-alignment right" href="http://fastml.com/coming-out/" title="Next Post: Coming out">Coming out »</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><iframe id="dsq-app1" name="dsq-app1" allowtransparency="true" scrolling="no" tabindex="0" title="Disqus" style="width: 1px ! important; min-width: 100% ! important; border: medium none ! important; overflow: hidden ! important; height: 1613px ! important;" src="Bayesian%20machine%20learning%20-%20FastML_files/a.htm" horizontalscrolling="no" verticalscrolling="no" frameborder="0" width="100%"></iframe></div>
  </section>

</div>

<aside class="sidebar thirds">
  
    <section class="first odd">
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="http://fastml.com/tuning-hyperparams-fast-with-hyperband/">Tuning hyperparams fast with Hyperband</a>
      </li>
    
      <li class="post">
        <a href="http://fastml.com/how-to-use-pd-dot-get-dummies-with-the-test-set/">How to use pd.get_dummies() with the test set</a>
      </li>
    
      <li class="post">
        <a href="http://fastml.com/data-in-predictions-out/">Data in, predictions out</a>
      </li>
    
      <li class="post">
        <a href="http://fastml.com/on-chatbots/">On chatbots</a>
      </li>
    
      <li class="post">
        <a href="http://fastml.com/piping-in-r-and-in-pandas/">Piping in R and in Pandas</a>
      </li>
    
      <li class="post">
        <a href="http://fastml.com/deep-learning-architecture-diagrams/">Deep learning architecture diagrams</a>
      </li>
    
      <li class="post">
        <a href="http://fastml.com/factorized-convolutional-neural-networks/">Factorized convolutional neural networks, AKA separable convolutions</a>
      </li>
    
  </ul>
</section>

<section class="even">
  <h1>Twitter</h1>
  
  <p>Follow <a href="http://twitter.com/fastml">@fastml</a> for notifications about new posts.</p>
  <ul id="tweets">
    <li class="loading">Status updating...</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("fastml", 0, true);
    });
  </script>
  <script src="Bayesian%20machine%20learning%20-%20FastML_files/twitter.js" type="text/javascript"> </script>
  <iframe id="twitter-widget-0" scrolling="no" allowtransparency="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 215px; height: 20px;" title="Twitter Follow Button" src="Bayesian%20machine%20learning%20-%20FastML_files/follow_button.htm" data-screen-name="fastml" frameborder="0"></iframe>

	<br><br>
	
	Also check out <a href="https://twitter.com/fastml_extra">@fastml_extra</a> for things related to machine learning and data science in general.
	
</section>


<section class="odd">
  <h1>GitHub</h1>
  
  <p>Most articles come with some <a href="http://fastml.com/blog/categories/code/">code</a>. We push it to Github.</p>
  
  
  <a href="https://github.com/zygmuntz">https://github.com/zygmuntz</a>
    

</section>

<section class="first even">
  <h1>Cubert</h1>
  
  <p>Visualize your data in interactive 3D, as described <a href="http://fastml.com/interactive-in-browser-3d-visualization-of-datasets/" style="text-decoration: underline;">here</a>.</p>
  
  <p><a href="http://cubert.fastml.com/">http://cubert.fastml.com/</a></p>

</section>


  
</aside>


    <span class="toggle-sidebar"></span></div>
  </div>
  <footer role="contentinfo"><p>
  Copyright © 2017 - Zygmunt Z. -
  <span class="credit">Powered by <a href="http://octopress.org/">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'fastml';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://fastml.com/bayesian-machine-learning/';
        var disqus_url = 'http://fastml.com/bayesian-machine-learning/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>





  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>


<script type="text/javascript">
	background = jQuery.cookie( "background" );	// set in head
	no_random_background = jQuery.cookie( "no_random_background" ); // set in backgrounds
	if ( background && no_random_background == 1 ) {
		set_background( background );
	}
</script>




<iframe name="oauth2relay424163203" id="oauth2relay424163203" src="Bayesian%20machine%20learning%20-%20FastML_files/postmessageRelay.htm" style="width: 1px; height: 1px; position: absolute; top: -100px;" tabindex="-1" aria-hidden="true"></iframe><iframe style="display: none;"></iframe><iframe id="rufous-sandbox" scrolling="no" allowtransparency="true" allowfullscreen="true" style="position: absolute; visibility: hidden; display: none; width: 0px; height: 0px; padding: 0px; border: medium none;" title="Twitter analytics iframe" frameborder="0"></iframe></body></html>